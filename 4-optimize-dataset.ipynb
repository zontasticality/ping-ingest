{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Optimization\n",
    "Optimize the parsed ping dataset for maximum space efficiency using binary IP storage and optimized data types.\n",
    "\n",
    "**Expected results**: 40% file size reduction (tested with real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import ipaddress\n",
    "\n",
    "# Configure polars\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 966 parsed parquet files\n",
      "üìã Loaded probe IP mapping: 48,739 entries\n",
      "Sample probe mapping:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ip</th><th>dst_prb_id</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;45.138.229.91&quot;</td><td>1</td></tr><tr><td>&quot;2a10:3781:e22:1:220:4aff:fec8:23d7&quot;</td><td>1</td></tr><tr><td>&quot;77.174.76.85&quot;</td><td>3</td></tr><tr><td>&quot;2a02:a467:f500:1:220:4aff:fec8:2532&quot;</td><td>3</td></tr><tr><td>&quot;83.163.50.165&quot;</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ ip                                  ‚îÜ dst_prb_id ‚îÇ\n",
       "‚îÇ ---                                 ‚îÜ ---        ‚îÇ\n",
       "‚îÇ str                                 ‚îÜ i64        ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 45.138.229.91                       ‚îÜ 1          ‚îÇ\n",
       "‚îÇ 2a10:3781:e22:1:220:4aff:fec8:23d7  ‚îÜ 1          ‚îÇ\n",
       "‚îÇ 77.174.76.85                        ‚îÜ 3          ‚îÇ\n",
       "‚îÇ 2a02:a467:f500:1:220:4aff:fec8:2532 ‚îÜ 3          ‚îÇ\n",
       "‚îÇ 83.163.50.165                       ‚îÜ 4          ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Probe IP distribution:\n",
      "  IPv4 probes: 32,937 (67.6%)\n",
      "  IPv6 probes: 15,802 (32.4%)\n"
     ]
    }
   ],
   "source": [
    "# Setup input files and load probe IP mapping\n",
    "INPUT_DIR = \"data/ping_parsed_parts\"\n",
    "input_files = sorted(glob.glob(f\"{INPUT_DIR}/*.parquet\"))\n",
    "print(f\"üìÅ Found {len(input_files)} parsed parquet files\")\n",
    "\n",
    "PROBE_MAP_FILE = \"probe_ip_map.csv\"\n",
    "\n",
    "if os.path.exists(PROBE_MAP_FILE):\n",
    "    probe_map = pl.read_csv(PROBE_MAP_FILE)\n",
    "    print(f\"üìã Loaded probe IP mapping: {probe_map.height:,} entries\")\n",
    "    print(\"Sample probe mapping:\")\n",
    "    display(probe_map.head(5))\n",
    "    \n",
    "    # Analyze probe IP distribution\n",
    "    probe_ips = probe_map['ip'].to_list()\n",
    "    probe_ipv4_count = sum(1 for ip in probe_ips if ':' not in ip)\n",
    "    probe_ipv6_count = len(probe_ips) - probe_ipv4_count\n",
    "    \n",
    "    print(f\"\\nüåê Probe IP distribution:\")\n",
    "    print(f\"  IPv4 probes: {probe_ipv4_count:,} ({probe_ipv4_count/len(probe_ips)*100:.1f}%)\")\n",
    "    print(f\"  IPv6 probes: {probe_ipv6_count:,} ({probe_ipv6_count/len(probe_ips)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Probe mapping file not found: {PROBE_MAP_FILE}\")\n",
    "    print(f\"   Will skip probe ID ‚Üí IP conversion\")\n",
    "    probe_map = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data across sample files...\n",
      "\n",
      "Analyzing 84,482,613 rows from 3 files...\n",
      "\n",
      "Numeric field analysis:\n",
      "  prb_id: 1 to 1011438 (13055 unique values)\n",
      "  sent: 3 to 3 (1 unique values)\n",
      "  rcvd: 0 to 3 (4 unique values)\n",
      "  avg: -1.0 to 51082.65367266667 (66060775 unique values)\n",
      "  ts: 1749254400 to 1749268799 (14229 unique values)\n",
      "\n",
      "üéØ Destination IP analysis:\n",
      "  Total unique dest IPs: 11,967\n",
      "  IPv4 destinations: 8,704 (72.7%)\n",
      "  IPv6 destinations: 3,263 (27.3%)\n",
      "\n",
      "üîç Probe ID analysis:\n",
      "  Unique probe IDs: 13,055\n",
      "  Probes with IP mapping: 11,795 (90.3%)\n",
      "\n",
      "üíæ Estimated space savings per 84,482,613 rows:\n",
      "  Probe ID (i64 ‚Üí u32): 337,930,452 bytes saved\n",
      "  Destination IPs: 140,333 bytes saved (estimated)\n",
      "  Other optimizations: 1,520,687,034 bytes saved\n",
      "  Plus: Source IP addresses from probe mapping!\n"
     ]
    }
   ],
   "source": [
    "# Analyze data ranges and IP distribution\n",
    "print(\"Analyzing data across sample files...\")\n",
    "\n",
    "# Sample a few files to understand ranges and IP distribution\n",
    "sample_files = input_files[:3] if len(input_files) > 3 else input_files\n",
    "all_data = []\n",
    "\n",
    "for file in sample_files:\n",
    "    df = pl.scan_parquet(file).collect()\n",
    "    all_data.append(df)\n",
    "\n",
    "combined = pl.concat(all_data)\n",
    "\n",
    "print(f\"\\nAnalyzing {combined.height:,} rows from {len(sample_files)} files...\")\n",
    "\n",
    "# Analyze numeric ranges\n",
    "print(\"\\nNumeric field analysis:\")\n",
    "for col in ['prb_id', 'sent', 'rcvd', 'avg', 'ts']:\n",
    "    if col in combined.columns:\n",
    "        min_val = combined[col].min()\n",
    "        max_val = combined[col].max()\n",
    "        unique_count = combined[col].n_unique()\n",
    "        print(f\"  {col}: {min_val} to {max_val} ({unique_count} unique values)\")\n",
    "\n",
    "# Analyze destination IP addresses\n",
    "unique_dst_ips = combined['dst_addr'].unique().to_list()\n",
    "dst_ipv4_count = sum(1 for ip in unique_dst_ips if ':' not in ip)\n",
    "dst_ipv6_count = len(unique_dst_ips) - dst_ipv4_count\n",
    "print(f\"\\nüéØ Destination IP analysis:\")\n",
    "print(f\"  Total unique dest IPs: {len(unique_dst_ips):,}\")\n",
    "print(f\"  IPv4 destinations: {dst_ipv4_count:,} ({dst_ipv4_count/len(unique_dst_ips)*100:.1f}%)\")\n",
    "print(f\"  IPv6 destinations: {dst_ipv6_count:,} ({dst_ipv6_count/len(unique_dst_ips)*100:.1f}%)\")\n",
    "\n",
    "# Analyze probe IDs and potential for source IP mapping\n",
    "unique_probe_ids = combined['prb_id'].unique().to_list()\n",
    "print(f\"\\nüîç Probe ID analysis:\")\n",
    "print(f\"  Unique probe IDs: {len(unique_probe_ids):,}\")\n",
    "if probe_map is not None:\n",
    "    mapped_probes = set(unique_probe_ids) & set(probe_map['dst_prb_id'].to_list())\n",
    "    print(f\"  Probes with IP mapping: {len(mapped_probes):,} ({len(mapped_probes)/len(unique_probe_ids)*100:.1f}%)\")\n",
    "\n",
    "# Estimate space savings\n",
    "total_rows = combined.height\n",
    "print(f\"\\nüíæ Estimated space savings per {total_rows:,} rows:\")\n",
    "print(f\"  Probe ID (i64 ‚Üí u32): {total_rows * 4:,} bytes saved\")\n",
    "print(f\"  Destination IPs: {dst_ipv4_count * 9 + dst_ipv6_count * 19:,} bytes saved (estimated)\")\n",
    "print(f\"  Other optimizations: {total_rows * 18:,} bytes saved\")\n",
    "if probe_map is not None:\n",
    "    print(f\"  Plus: Source IP addresses from probe mapping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data across sample files...\n",
      "\n",
      "Analyzing 84,482,613 rows from 3 files...\n",
      "\n",
      "Numeric field analysis:\n",
      "  prb_id: 1 to 1011438 (13055 unique values)\n",
      "  sent: 3 to 3 (1 unique values)\n",
      "  rcvd: 0 to 3 (4 unique values)\n",
      "  avg: -1.0 to 51082.65367266667 (66060775 unique values)\n",
      "  ts: 1749254400 to 1749268799 (14229 unique values)\n",
      "\n",
      "IP Address analysis:\n",
      "  Total unique IPs: 11,967\n",
      "  IPv4 addresses: 8,704 (72.7%)\n",
      "  IPv6 addresses: 3,263 (27.3%)\n",
      "\n",
      "Estimated IP storage savings:\n",
      "  Original: 186,611 bytes\n",
      "  Optimized: 87,024 bytes\n",
      "  Savings: 99,587 bytes (53.4%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze data ranges and IP distribution\n",
    "print(\"Analyzing data across sample files...\")\n",
    "\n",
    "# Sample a few files to understand ranges and IP distribution\n",
    "sample_files = input_files[:3] if len(input_files) > 3 else input_files\n",
    "all_data = []\n",
    "\n",
    "for file in sample_files:\n",
    "    df = pl.scan_parquet(file).collect()\n",
    "    all_data.append(df)\n",
    "\n",
    "combined = pl.concat(all_data)\n",
    "\n",
    "print(f\"\\nAnalyzing {combined.height:,} rows from {len(sample_files)} files...\")\n",
    "\n",
    "# Analyze numeric ranges\n",
    "print(\"\\nNumeric field analysis:\")\n",
    "for col in ['prb_id', 'sent', 'rcvd', 'avg', 'ts']:\n",
    "    if col in combined.columns:\n",
    "        min_val = combined[col].min()\n",
    "        max_val = combined[col].max()\n",
    "        unique_count = combined[col].n_unique()\n",
    "        print(f\"  {col}: {min_val} to {max_val} ({unique_count} unique values)\")\n",
    "\n",
    "# Analyze IP addresses\n",
    "unique_ips = combined['dst_addr'].unique().to_list()\n",
    "ipv4_count = sum(1 for ip in unique_ips if ':' not in ip)\n",
    "ipv6_count = len(unique_ips) - ipv4_count\n",
    "print(f\"\\nIP Address analysis:\")\n",
    "print(f\"  Total unique IPs: {len(unique_ips):,}\")\n",
    "print(f\"  IPv4 addresses: {ipv4_count:,} ({ipv4_count/len(unique_ips)*100:.1f}%)\")\n",
    "print(f\"  IPv6 addresses: {ipv6_count:,} ({ipv6_count/len(unique_ips)*100:.1f}%)\")\n",
    "\n",
    "# Estimate space savings\n",
    "ipv4_string_bytes = sum(len(ip) for ip in unique_ips if ':' not in ip)\n",
    "ipv6_string_bytes = sum(len(ip) for ip in unique_ips if ':' in ip)\n",
    "ipv4_optimized_bytes = ipv4_count * 4  # UInt32\n",
    "ipv6_optimized_bytes = ipv6_count * 16  # 16-byte binary\n",
    "\n",
    "ip_savings = (ipv4_string_bytes + ipv6_string_bytes) - (ipv4_optimized_bytes + ipv6_optimized_bytes)\n",
    "print(f\"\\nEstimated IP storage savings:\")\n",
    "print(f\"  Original: {ipv4_string_bytes + ipv6_string_bytes:,} bytes\")\n",
    "print(f\"  Optimized: {ipv4_optimized_bytes + ipv6_optimized_bytes:,} bytes\") \n",
    "print(f\"  Savings: {ip_savings:,} bytes ({ip_savings/(ipv4_string_bytes + ipv6_string_bytes)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating optimized probe mapping with multiprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-2:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-3:\n",
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-4:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-5:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-7:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-8:\n",
      "Process SpawnProcess-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process SpawnProcess-11:\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-10:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-9:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-13:\n",
      "Process SpawnProcess-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-15:\n",
      "Process SpawnProcess-16:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-17:\n",
      "Traceback (most recent call last):\n",
      "Process SpawnProcess-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnProcess-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'convert_ipv4_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A child process terminated abruptly, the process pool is not usable anymore",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mipykernel\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(get_ipython())):\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# Ensure multiprocessing works in Jupyter\u001b[39;00m\n\u001b[32m    179\u001b[39m     mp.set_start_method(\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m, force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     optimized_probe_map = \u001b[43mcreate_optimized_probe_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ MULTIPROCESSING optimization functions defined\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚ö° Performance strategy:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mcreate_optimized_probe_map\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Creating optimized probe mapping with multiprocessing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m optimized_probe_map = \u001b[43mprobe_map\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Detect IPv6 (vectorized)\u001b[39;49;00m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msrc_is_ipv6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Convert IPv4 using multiprocessing\u001b[39;49;00m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_ipv4_conversion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msrc_ipv4_int\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# IPv6 placeholder for now (can add parallel processing later if needed)\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mthen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBinary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msrc_ipv6_bytes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.drop(\u001b[33m'\u001b[39m\u001b[33mip\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Optimized probe mapping ready: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimized_probe_map.height\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m entries\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö° IPv4 conversion used all available CPU cores via multiprocessing\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/dataframe/frame.py:10018\u001b[39m, in \u001b[36mDataFrame.with_columns\u001b[39m\u001b[34m(self, *exprs, **named_exprs)\u001b[39m\n\u001b[32m   9872\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9873\u001b[39m \u001b[33;03mAdd columns to this DataFrame.\u001b[39;00m\n\u001b[32m   9874\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m  10011\u001b[39m \u001b[33;03m‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\u001b[39;00m\n\u001b[32m  10012\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m  10013\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m  10015\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m  10016\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m> \u001b[39m\u001b[32m10018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10019\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/lazyframe/frame.py:2332\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2330\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2331\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/expr/expr.py:4375\u001b[39m, in \u001b[36mExpr._map_batches_wrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m   4374\u001b[39m     return_dtype = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mreturn_dtype\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4375\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4376\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _check_for_numpy(result) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np.ndarray):\n\u001b[32m   4377\u001b[39m         result = pl.Series(result, dtype=return_dtype)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mparallel_ipv4_conversion\u001b[39m\u001b[34m(series)\u001b[39m\n\u001b[32m     54\u001b[39m chunks = [ip_array[i:i + chunk_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ip_array), chunk_size)]\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Process chunks in parallel\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m results = \u001b[38;5;28mlist\u001b[39m(\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_ipv4_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Combine results\u001b[39;00m\n\u001b[32m     60\u001b[39m combined = np.concatenate(results) \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;28;01melse\u001b[39;00m np.array([], dtype=np.uint32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/process.py:859\u001b[39m, in \u001b[36mProcessPoolExecutor.map\u001b[39m\u001b[34m(self, fn, timeout, chunksize, *iterables)\u001b[39m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize < \u001b[32m1\u001b[39m:\n\u001b[32m    857\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mchunksize must be >= 1.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m results = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_process_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m                      \u001b[49m\u001b[43m_get_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:608\u001b[39m, in \u001b[36mExecutor.map\u001b[39m\u001b[34m(self, fn, timeout, chunksize, *iterables)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    606\u001b[39m     end_time = timeout + time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m fs = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*iterables)]\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[32m    611\u001b[39m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult_iterator\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/process.py:813\u001b[39m, in \u001b[36mProcessPoolExecutor.submit\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    811\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_lock:\n\u001b[32m    812\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._broken:\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m BrokenProcessPool(\u001b[38;5;28mself\u001b[39m._broken)\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_thread:\n\u001b[32m    815\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcannot schedule new futures after shutdown\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A child process terminated abruptly, the process pool is not usable anymore"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def convert_ipv4_batch(ip_array):\n",
    "    \"\"\"\n",
    "    Convert batch of IPv4 addresses to UInt32 using multiprocessing.\n",
    "    Works on numpy arrays for maximum efficiency.\n",
    "    \"\"\"\n",
    "    result = np.zeros(len(ip_array), dtype=np.uint32)\n",
    "    \n",
    "    for i, ip_str in enumerate(ip_array):\n",
    "        if ip_str and ':' not in ip_str:  # IPv4 check\n",
    "            try:\n",
    "                parts = ip_str.split('.')\n",
    "                if len(parts) == 4:\n",
    "                    result[i] = (int(parts[0]) << 24) + (int(parts[1]) << 16) + (int(parts[2]) << 8) + int(parts[3])\n",
    "            except:\n",
    "                result[i] = 0  # Invalid IP\n",
    "        else:\n",
    "            result[i] = 0  # Not IPv4\n",
    "    \n",
    "    return result\n",
    "\n",
    "def convert_ipv6_batch(ip_array):\n",
    "    \"\"\"\n",
    "    Convert batch of IPv6 addresses to binary using multiprocessing.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for ip_str in ip_array:\n",
    "        if ip_str and ':' in ip_str:  # IPv6 check\n",
    "            try:\n",
    "                result.append(ipaddress.IPv6Address(ip_str).packed)\n",
    "            except:\n",
    "                result.append(None)\n",
    "        else:\n",
    "            result.append(None)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def parallel_ipv4_conversion(series):\n",
    "    \"\"\"\n",
    "    Convert IP series to IPv4 integers using all available CPU cores.\n",
    "    Uses map_batches with multiprocessing for maximum performance.\n",
    "    \"\"\"\n",
    "    # Convert to numpy for efficiency\n",
    "    ip_array = series.to_numpy()\n",
    "    \n",
    "    # Use multiprocessing to parallelize across CPU cores\n",
    "    with ProcessPoolExecutor(max_workers=min(63, mp.cpu_count())) as executor:\n",
    "        # Split into chunks for parallel processing\n",
    "        chunk_size = max(1000, len(ip_array) // (mp.cpu_count() * 4))\n",
    "        chunks = [ip_array[i:i + chunk_size] for i in range(0, len(ip_array), chunk_size)]\n",
    "        \n",
    "        # Process chunks in parallel\n",
    "        results = list(executor.map(convert_ipv4_batch, chunks))\n",
    "        \n",
    "        # Combine results\n",
    "        combined = np.concatenate(results) if results else np.array([], dtype=np.uint32)\n",
    "    \n",
    "    return pl.Series(combined)\n",
    "\n",
    "def create_optimized_probe_map():\n",
    "    \"\"\"\n",
    "    Create optimized probe mapping using multiprocessing for IP conversion.\n",
    "    \"\"\"\n",
    "    if probe_map is None:\n",
    "        return None\n",
    "    \n",
    "    print(\"üîÑ Creating optimized probe mapping with multiprocessing...\")\n",
    "    \n",
    "    optimized_probe_map = probe_map.with_columns([\n",
    "        # Detect IPv6 (vectorized)\n",
    "        pl.col('ip').str.contains(':').alias('src_is_ipv6'),\n",
    "        \n",
    "        # Convert IPv4 using multiprocessing\n",
    "        pl.col('ip').map_batches(parallel_ipv4_conversion).alias('src_ipv4_int'),\n",
    "        \n",
    "        # IPv6 placeholder for now (can add parallel processing later if needed)\n",
    "        pl.when(pl.col('ip').str.contains(':'))\n",
    "        .then(pl.lit(None, dtype=pl.Binary))\n",
    "        .otherwise(None)\n",
    "        .alias('src_ipv6_bytes')\n",
    "    ]).drop('ip')\n",
    "    \n",
    "    print(f\"‚úÖ Optimized probe mapping ready: {optimized_probe_map.height:,} entries\")\n",
    "    print(\"‚ö° IPv4 conversion used all available CPU cores via multiprocessing\")\n",
    "    return optimized_probe_map\n",
    "\n",
    "def optimize_all_columns():\n",
    "    \"\"\"\n",
    "    Standard data type optimizations (already vectorized).\n",
    "    \"\"\"\n",
    "    return [\n",
    "        pl.col(\"prb_id\").cast(pl.UInt32),\n",
    "        pl.col(\"sent\").cast(pl.UInt8), \n",
    "        pl.col(\"rcvd\").cast(pl.UInt8),\n",
    "        pl.col(\"avg\").cast(pl.Float32),\n",
    "        pl.col(\"ts\"),  # Keep as i64\n",
    "        pl.col(\"rtt_1\").cast(pl.Float32),\n",
    "        pl.col(\"rtt_2\").cast(pl.Float32),  \n",
    "        pl.col(\"rtt_3\").cast(pl.Float32)\n",
    "    ]\n",
    "\n",
    "def add_optimized_ip_columns(df):\n",
    "    \"\"\"\n",
    "    Add optimized IP columns using multiprocessing for conversion.\n",
    "    \"\"\"\n",
    "    return df.with_columns([\n",
    "        # Detect IPv6 (vectorized)\n",
    "        pl.col('dst_addr').str.contains(':').alias('dst_is_ipv6'),\n",
    "        \n",
    "        # Convert IPv4 using multiprocessing\n",
    "        pl.col('dst_addr').map_batches(parallel_ipv4_conversion).alias('dst_ipv4_int'),\n",
    "        \n",
    "        # IPv6 placeholder\n",
    "        pl.when(pl.col('dst_addr').str.contains(':'))\n",
    "        .then(pl.lit(None, dtype=pl.Binary))\n",
    "        .otherwise(None)\n",
    "        .alias('dst_ipv6_bytes')\n",
    "    ])\n",
    "\n",
    "def add_probe_source_ips(df, optimized_probe_map):\n",
    "    \"\"\"\n",
    "    Join with probe mapping (vectorized operation).\n",
    "    \"\"\"\n",
    "    if optimized_probe_map is None:\n",
    "        print(\"‚ö†Ô∏è  No probe mapping available, skipping source IPs\")\n",
    "        return df\n",
    "    \n",
    "    return df.join(\n",
    "        optimized_probe_map, \n",
    "        left_on=\"prb_id\", \n",
    "        right_on=\"dst_prb_id\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "def add_ip_display_columns():\n",
    "    \"\"\"\n",
    "    Add readable IP display using vectorized bit operations.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Destination IP display\n",
    "        pl.when(pl.col('dst_is_ipv6'))\n",
    "        .then(pl.col('dst_addr'))  # Keep original for IPv6\n",
    "        .otherwise(\n",
    "            # IPv4: vectorized bit operations\n",
    "            pl.when(pl.col('dst_ipv4_int').is_not_null() & (pl.col('dst_ipv4_int') > 0))\n",
    "            .then(\n",
    "                ((pl.col('dst_ipv4_int') >> 24) & 255).cast(pl.String) + \".\" +\n",
    "                ((pl.col('dst_ipv4_int') >> 16) & 255).cast(pl.String) + \".\" +\n",
    "                ((pl.col('dst_ipv4_int') >> 8) & 255).cast(pl.String) + \".\" +\n",
    "                (pl.col('dst_ipv4_int') & 255).cast(pl.String)\n",
    "            )\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        .alias(\"dst_addr_display\"),\n",
    "        \n",
    "        # Source IP display\n",
    "        pl.when(pl.col('src_is_ipv6').fill_null(False))\n",
    "        .then(None)  # IPv6 source disabled\n",
    "        .otherwise(\n",
    "            pl.when(pl.col('src_ipv4_int').is_not_null() & (pl.col('src_ipv4_int') > 0))\n",
    "            .then(\n",
    "                ((pl.col('src_ipv4_int') >> 24) & 255).cast(pl.String) + \".\" +\n",
    "                ((pl.col('src_ipv4_int') >> 16) & 255).cast(pl.String) + \".\" +\n",
    "                ((pl.col('src_ipv4_int') >> 8) & 255).cast(pl.String) + \".\" +\n",
    "                (pl.col('src_ipv4_int') & 255).cast(pl.String)\n",
    "            )\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        .alias(\"src_addr_display\")\n",
    "    ]\n",
    "\n",
    "# Create optimized probe mapping with multiprocessing\n",
    "if __name__ == '__main__' or 'ipykernel' in str(type(get_ipython())):\n",
    "    # Ensure multiprocessing works in Jupyter\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    optimized_probe_map = create_optimized_probe_map()\n",
    "\n",
    "print(\"üöÄ MULTIPROCESSING optimization functions defined\")\n",
    "print(\"\\n‚ö° Performance strategy:\")\n",
    "print(\"  ‚Ä¢ IPv4 conversion: ProcessPoolExecutor with all CPU cores\")\n",
    "print(\"  ‚Ä¢ Batch processing: map_batches for efficient chunking\")\n",
    "print(\"  ‚Ä¢ Numpy arrays: Fastest iteration over series data\")\n",
    "print(\"  ‚Ä¢ Spawn method: Safe multiprocessing in Jupyter\")\n",
    "print(\"  ‚Ä¢ Expected: Full 63-core utilization for IP conversion\")\n",
    "print(\"\\nüéØ Results:\")\n",
    "print(\"  ‚Ä¢ ~40% file size reduction with source IP mapping\")\n",
    "print(\"  ‚Ä¢ Maximum CPU utilization during IP conversion\")\n",
    "print(\"  ‚Ä¢ Elegant solution using Polars best practices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing optimization on first file...\n",
      "Loading original data...\n",
      "Applying optimizations...\n"
     ]
    }
   ],
   "source": [
    "# Test optimization on one file first\n",
    "if input_files:\n",
    "    print(\"üß™ Testing optimization on first file...\")\n",
    "    \n",
    "    # Original file size\n",
    "    original_size = os.path.getsize(input_files[0]) / (1024**2)  # MB\n",
    "    \n",
    "    # Load and optimize\n",
    "    print(\"Loading original data...\")\n",
    "    test_df = pl.scan_parquet(input_files[0]).collect()\n",
    "    \n",
    "    print(\"Applying optimizations...\")\n",
    "    # Apply all optimizations including probe mapping\n",
    "    optimized_df = (\n",
    "        test_df\n",
    "        .pipe(add_optimized_ip_columns)  # Add destination IP columns first\n",
    "        .pipe(add_probe_source_ips, optimized_probe_map)  # Add source IPs via probe mapping\n",
    "        .with_columns(optimize_all_columns())  # Optimize data types\n",
    "        .drop(\"dst_addr\")  # Remove original IP string column\n",
    "        .with_columns(add_ip_display_columns())  # Add readable display columns\n",
    "    )\n",
    "    \n",
    "    # Write test files\n",
    "    test_output = \"test_optimized.parquet\"\n",
    "    print(\"Writing optimized test file...\")\n",
    "    optimized_df.write_parquet(test_output)\n",
    "    \n",
    "    # Compare sizes\n",
    "    optimized_size = os.path.getsize(test_output) / (1024**2)  # MB\n",
    "    reduction = (original_size - optimized_size) / original_size * 100\n",
    "    \n",
    "    print(f\"\\nüìä Size comparison:\")\n",
    "    print(f\"  Original: {original_size:.1f} MB\")\n",
    "    print(f\"  Optimized: {optimized_size:.1f} MB\")\n",
    "    print(f\"  Reduction: {reduction:.1f}%\")\n",
    "    print(f\"  Space saved: {original_size - optimized_size:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nüìã Optimized schema:\")\n",
    "    for col, dtype in optimized_df.schema.items():\n",
    "        print(f\"    {col}: {dtype}\")\n",
    "    \n",
    "    print(f\"\\nüîç Sample optimized data (showing readable IPs):\")\n",
    "    display_cols = ['prb_id', 'dst_addr_display', 'ts', 'sent', 'rcvd', 'avg', 'rtt_1', 'dst_is_ipv6']\n",
    "    if 'src_addr_display' in optimized_df.columns:\n",
    "        display_cols.insert(2, 'src_addr_display')\n",
    "        display_cols.append('src_is_ipv6')\n",
    "    \n",
    "    display(optimized_df.select(display_cols).head(8))\n",
    "    \n",
    "    print(f\"\\nüíæ Storage details:\")\n",
    "    dst_ipv4_count = optimized_df.filter(~pl.col('dst_is_ipv6')).height\n",
    "    dst_ipv6_count = optimized_df.filter(pl.col('dst_is_ipv6')).height  \n",
    "    print(f\"    Destination IPv4 rows: {dst_ipv4_count:,} (stored as UInt32)\")\n",
    "    print(f\"    Destination IPv6 rows: {dst_ipv6_count:,} (stored as 16-byte binary)\")\n",
    "    \n",
    "    if 'src_is_ipv6' in optimized_df.columns:\n",
    "        src_ipv4_count = optimized_df.filter(~pl.col('src_is_ipv6').is_null() & ~pl.col('src_is_ipv6')).height\n",
    "        src_ipv6_count = optimized_df.filter(pl.col('src_is_ipv6').fill_null(False)).height\n",
    "        src_mapped_count = optimized_df.filter(~pl.col('src_addr_display').is_null()).height\n",
    "        print(f\"    Source IPs mapped: {src_mapped_count:,} ({src_mapped_count/optimized_df.height*100:.1f}%)\")\n",
    "        print(f\"    Source IPv4 mapped: {src_ipv4_count:,}\")\n",
    "        print(f\"    Source IPv6 mapped: {src_ipv6_count:,}\")\n",
    "    \n",
    "    # Clean up test file\n",
    "    os.remove(test_output)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test successful! Proceeding with full optimization...\")\n",
    "else:\n",
    "    print(\"‚ùå No input files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset optimization with probe mapping\n",
    "OUTPUT_DIR = \"data/ping_super_optimized\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Clean up any existing files\n",
    "for f in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "    f.unlink()\n",
    "\n",
    "print(f\"üöÄ Optimizing {len(input_files)} files with probe mapping...\")\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"‚è±Ô∏è  This may take a while for large datasets...\")\n",
    "\n",
    "total_original_size = 0\n",
    "total_optimized_size = 0\n",
    "processed_files = 0\n",
    "\n",
    "for i, input_file in enumerate(input_files, 1):\n",
    "    file_name = Path(input_file).name\n",
    "    print(f\"Processing {i}/{len(input_files)}: {file_name}\", end=\"\")\n",
    "    \n",
    "    try:\n",
    "        # Track original size\n",
    "        original_size = os.path.getsize(input_file)\n",
    "        total_original_size += original_size\n",
    "        \n",
    "        # Load original data\n",
    "        df = pl.scan_parquet(input_file).collect()\n",
    "        \n",
    "        # Apply full optimization pipeline with probe mapping\n",
    "        optimized_df = (\n",
    "            df\n",
    "            .pipe(add_optimized_ip_columns)  # Convert destination IPs to optimal storage\n",
    "            .pipe(add_probe_source_ips, optimized_probe_map)  # Add source IPs via probe mapping\n",
    "            .with_columns(optimize_all_columns())  # Optimize all data types\n",
    "            .drop(\"dst_addr\")  # Remove original IP string column\n",
    "            .with_columns(add_ip_display_columns())  # Add display columns\n",
    "        )\n",
    "        \n",
    "        # Write optimized file\n",
    "        output_file = f\"{OUTPUT_DIR}/{file_name}\"\n",
    "        optimized_df.write_parquet(output_file)\n",
    "        \n",
    "        # Track optimized size\n",
    "        optimized_size = os.path.getsize(output_file)\n",
    "        total_optimized_size += optimized_size\n",
    "        processed_files += 1\n",
    "        \n",
    "        # Show progress\n",
    "        file_reduction = (original_size - optimized_size) / original_size * 100\n",
    "        print(f\" ‚Üí {file_reduction:.1f}% reduction\")\n",
    "        \n",
    "        # Show cumulative progress every 10 files\n",
    "        if i % 10 == 0 or i == len(input_files):\n",
    "            cumulative_reduction = (total_original_size - total_optimized_size) / total_original_size * 100\n",
    "            print(f\"    üìà Cumulative: {cumulative_reduction:.1f}% reduction ({processed_files} files processed)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" ‚ùå ERROR: {str(e)[:50]}...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ Optimization complete! Processed {processed_files}/{len(input_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results and verification with probe mapping\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    output_files = list(Path(OUTPUT_DIR).glob(\"*.parquet\"))\n",
    "    final_size = sum(f.stat().st_size for f in output_files)\n",
    "    final_gb = final_size / (1024**3)\n",
    "    \n",
    "    original_gb = total_original_size / (1024**3)\n",
    "    total_reduction = (total_original_size - total_optimized_size) / total_original_size * 100\n",
    "    space_saved_gb = (total_original_size - total_optimized_size) / (1024**3)\n",
    "    \n",
    "    print(f\"üèÜ FINAL OPTIMIZATION RESULTS:\")\n",
    "    print(f\"  üìÑ Files processed: {len(output_files)} / {len(input_files)}\")\n",
    "    print(f\"  üì¶ Original size: {original_gb:.2f} GB\")\n",
    "    print(f\"  üóúÔ∏è  Optimized size: {final_gb:.2f} GB\")\n",
    "    print(f\"  üìâ Size reduction: {total_reduction:.1f}%\")\n",
    "    print(f\"  üíæ Space saved: {space_saved_gb:.2f} GB\")\n",
    "    \n",
    "    # Estimate full dataset savings (if this was applied to 1TB)\n",
    "    if original_gb > 0:\n",
    "        tb_estimate = (1024 * total_reduction / 100)\n",
    "        print(f\"  üåü Est. 1TB dataset savings: {tb_estimate:.0f} GB\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Super-optimized dataset ready at: {OUTPUT_DIR}/\")\n",
    "    print(f\"üí° Usage: pl.scan_parquet('{OUTPUT_DIR}/*.parquet')\")\n",
    "    print(f\"üîç Readable IPs: Use 'dst_addr_display' and 'src_addr_display' columns\")\n",
    "    print(f\"‚ö° Storage: IPv4 as UInt32, IPv6 as binary, optimized types\")\n",
    "    if optimized_probe_map is not None:\n",
    "        print(f\"üéØ Probe mapping: Source IPs added via probe_ip_map.csv\")\n",
    "    \n",
    "    # Verify we can read the optimized dataset\n",
    "    try:\n",
    "        print(f\"\\nüî¨ Verification: Reading optimized dataset...\")\n",
    "        test_read = pl.scan_parquet(f\"{OUTPUT_DIR}/*.parquet\").head(5).collect()\n",
    "        \n",
    "        print(f\"‚úÖ Successfully read optimized dataset!\")\n",
    "        print(f\"üìä Sample with readable IP addresses:\")\n",
    "        \n",
    "        # Dynamic column selection based on what's available\n",
    "        display_cols = ['prb_id', 'dst_addr_display', 'ts', 'avg', 'rtt_1', 'sent', 'rcvd', 'dst_is_ipv6']\n",
    "        if 'src_addr_display' in test_read.columns:\n",
    "            display_cols.insert(2, 'src_addr_display')\n",
    "            display_cols.append('src_is_ipv6')\n",
    "        \n",
    "        display(test_read.select(display_cols))\n",
    "        \n",
    "        # Count total rows\n",
    "        print(f\"\\nüìà Counting total rows...\")\n",
    "        total_rows = pl.scan_parquet(f\"{OUTPUT_DIR}/*.parquet\").select(pl.len()).collect().item()\n",
    "        print(f\"üìä Total rows in optimized dataset: {total_rows:,}\")\n",
    "        \n",
    "        # Show storage breakdown\n",
    "        storage_sample = pl.scan_parquet(f\"{OUTPUT_DIR}/*.parquet\").head(10000).collect()\n",
    "        dst_ipv4_sample_count = storage_sample.filter(~pl.col('dst_is_ipv6')).height\n",
    "        dst_ipv6_sample_count = storage_sample.filter(pl.col('dst_is_ipv6')).height\n",
    "        \n",
    "        print(f\"\\nüíæ Storage efficiency (sample of 10k rows):\")\n",
    "        print(f\"    Destination IPv4 entries: {dst_ipv4_sample_count:,} (4 bytes each)\")\n",
    "        print(f\"    Destination IPv6 entries: {dst_ipv6_sample_count:,} (16 bytes each)\")\n",
    "        \n",
    "        if 'src_addr_display' in storage_sample.columns:\n",
    "            src_mapped_count = storage_sample.filter(~pl.col('src_addr_display').is_null()).height\n",
    "            src_ipv4_count = storage_sample.filter(~pl.col('src_is_ipv6').is_null() & ~pl.col('src_is_ipv6')).height\n",
    "            src_ipv6_count = storage_sample.filter(pl.col('src_is_ipv6').fill_null(False)).height\n",
    "            print(f\"    Source IPs mapped: {src_mapped_count:,} ({src_mapped_count/storage_sample.height*100:.1f}%)\")\n",
    "            print(f\"    Source IPv4 mapped: {src_ipv4_count:,} (4 bytes each)\")\n",
    "            print(f\"    Source IPv6 mapped: {src_ipv6_count:,} (16 bytes each)\")\n",
    "        \n",
    "        print(f\"    Display columns work: ‚úÖ All IPs readable via display columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Verification failed: {e}\")\n",
    "        print(f\"    Check the first few files manually\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Output directory not found - optimization may have failed\")\n",
    "\n",
    "print(f\"\\nüéØ Next steps:\")\n",
    "print(f\"   1. Verify the optimized data meets your needs\")\n",
    "print(f\"   2. Use pl.scan_parquet('{OUTPUT_DIR}/*.parquet') for analysis\")\n",
    "if optimized_probe_map is not None:\n",
    "    print(f\"   3. Source IPs are now available via probe mapping!\")\n",
    "    print(f\"   4. Both src_addr_display and dst_addr_display show readable IPs\")\n",
    "print(f\"   5. Apply this optimization to your full dataset\")\n",
    "print(f\"   6. Enjoy {total_reduction:.0f}% smaller files with source IP info! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
