{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c38e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyright: basic, reportUnknownVariableType=false, reportUnknownMemberType=false\n",
    "\n",
    "import polars as pl\n",
    "import json\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Configure polars for better performance\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3467ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rtts_native(n=3):\n",
    "    \"\"\"\n",
    "    Returns a list of Polars expressions to parse a JSON 'result' column\n",
    "    and extract the first 'n' RTT values into separate columns.\n",
    "\n",
    "    This version uses str.json_path_match to robustly handle mixed JSON schemas\n",
    "    (e.g., some with 'rtt', others with 'x') without schema inference errors.\n",
    "    \"\"\"\n",
    "    # 1. Use json_path_match to find all 'rtt' values in the list.\n",
    "    #    The JSONPath `r\"$[*].rtt\"` means:\n",
    "    #    - `$`     : Start at the root.\n",
    "    #    - `[*]`   : Get all elements in the top-level array.\n",
    "    #    - `.rtt`  : From each element, extract the value of the \"rtt\" key.\n",
    "    #    This returns a List of Strings, e.g., [\"25.519\", \"25.671\", \"25.801\"].\n",
    "    #    If no 'rtt' keys are found, it correctly returns an empty list or null.\n",
    "    #    THIS IS THE KEY FIX, as it bypasses schema validation.\n",
    "    rtt_list_expr = pl.col(\"result\").str.json_path_match(r\"$[*].rtt\")\n",
    "\n",
    "    # 2. The result of json_path_match is a list of STRINGS. We must cast them to floats.\n",
    "    #    We use list.eval to run an expression on each element of the list.\n",
    "    #    `strict=False` will turn any non-numeric values into null instead of erroring.\n",
    "    rtt_floats_expr = rtt_list_expr.list.eval(\n",
    "        pl.element().cast(pl.Float64, strict=False)\n",
    "    )\n",
    "\n",
    "    # 3. Create 'n' new columns by getting elements from the list of RTTs.\n",
    "    #    This part remains the same.\n",
    "    rtt_columns = [\n",
    "        rtt_floats_expr.list.get(i).alias(f\"rtt_{i+1}\")\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "    return rtt_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28e5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96562 files to process.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_GLOB = \"data/ping/**/*.parquet\"\n",
    "OUTPUT_PATH = \"data/ping_parsed.parquet\"\n",
    "BATCH_SIZE = 100  # Process 1000 files at a time. Adjust as needed.\n",
    "N_RTTS = 3\n",
    "\n",
    "# Get a list of all file paths\n",
    "all_files = glob.glob(SOURCE_GLOB)\n",
    "print(f\"Found {len(all_files)} files to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51266937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (100 files)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>prb_id</th><th>dst_addr</th><th>ts</th><th>sent</th><th>rcvd</th><th>avg</th><th>result</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>6878</td><td>&quot;91.243.43.19&quot;</td><td>1749256784</td><td>3</td><td>0</td><td>-1.0</td><td>&quot;[{&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}]&quot;</td></tr><tr><td>6878</td><td>&quot;2001:4ba0:ffe0:ffff::4&quot;</td><td>1749256785</td><td>3</td><td>3</td><td>25.62511</td><td>&quot;[{&quot;rtt&quot;: 25.519027}, {&quot;rtt&quot;: 25.671107}, {&quot;rtt&quot;: 2…</td></tr><tr><td>6878</td><td>&quot;213.91.165.187&quot;</td><td>1749256786</td><td>3</td><td>3</td><td>65.974445</td><td>&quot;[{&quot;rtt&quot;: 66.019935}, {&quot;rtt&quot;: 65.976451}, {&quot;rtt&quot;: 6…</td></tr><tr><td>6878</td><td>&quot;146.185.219.73&quot;</td><td>1749256787</td><td>3</td><td>3</td><td>86.520995</td><td>&quot;[{&quot;rtt&quot;: 86.613995}, {&quot;rtt&quot;: 86.530067}, {&quot;rtt&quot;: 8…</td></tr><tr><td>6878</td><td>&quot;2a01:9e01:4d05:3333::a&quot;</td><td>1749256789</td><td>3</td><td>0</td><td>-1.0</td><td>&quot;[{&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}]&quot;</td></tr><tr><td>6878</td><td>&quot;69.30.249.206&quot;</td><td>1749256789</td><td>3</td><td>3</td><td>148.248946</td><td>&quot;[{&quot;rtt&quot;: 148.23621}, {&quot;rtt&quot;: 148.301316}, {&quot;rtt&quot;: …</td></tr><tr><td>6878</td><td>&quot;45.41.55.165&quot;</td><td>1749256789</td><td>3</td><td>0</td><td>-1.0</td><td>&quot;[{&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}]&quot;</td></tr><tr><td>6878</td><td>&quot;92.38.176.25&quot;</td><td>1749256790</td><td>3</td><td>3</td><td>126.80081</td><td>&quot;[{&quot;rtt&quot;: 126.767651}, {&quot;rtt&quot;: 126.777332}, {&quot;rtt&quot;:…</td></tr><tr><td>6878</td><td>&quot;212.62.68.29&quot;</td><td>1749256793</td><td>3</td><td>3</td><td>29.075086</td><td>&quot;[{&quot;rtt&quot;: 29.087843}, {&quot;rtt&quot;: 29.118559}, {&quot;rtt&quot;: 2…</td></tr><tr><td>6878</td><td>&quot;34.89.240.90&quot;</td><td>1749256797</td><td>3</td><td>0</td><td>-1.0</td><td>&quot;[{&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}, {&quot;x&quot;: &quot;*&quot;}]&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 7)\n",
       "┌────────┬────────────────────────┬────────────┬──────┬──────┬────────────┬────────────────────────┐\n",
       "│ prb_id ┆ dst_addr               ┆ ts         ┆ sent ┆ rcvd ┆ avg        ┆ result                 │\n",
       "│ ---    ┆ ---                    ┆ ---        ┆ ---  ┆ ---  ┆ ---        ┆ ---                    │\n",
       "│ i64    ┆ str                    ┆ i64        ┆ i64  ┆ i64  ┆ f64        ┆ str                    │\n",
       "╞════════╪════════════════════════╪════════════╪══════╪══════╪════════════╪════════════════════════╡\n",
       "│ 6878   ┆ 91.243.43.19           ┆ 1749256784 ┆ 3    ┆ 0    ┆ -1.0       ┆ [{\"x\": \"*\"}, {\"x\":     │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ \"*\"}, {\"x\": \"*\"}]      │\n",
       "│ 6878   ┆ 2001:4ba0:ffe0:ffff::4 ┆ 1749256785 ┆ 3    ┆ 3    ┆ 25.62511   ┆ [{\"rtt\": 25.519027},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 25.671107},    │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 2…             │\n",
       "│ 6878   ┆ 213.91.165.187         ┆ 1749256786 ┆ 3    ┆ 3    ┆ 65.974445  ┆ [{\"rtt\": 66.019935},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 65.976451},    │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 6…             │\n",
       "│ 6878   ┆ 146.185.219.73         ┆ 1749256787 ┆ 3    ┆ 3    ┆ 86.520995  ┆ [{\"rtt\": 86.613995},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 86.530067},    │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 8…             │\n",
       "│ 6878   ┆ 2a01:9e01:4d05:3333::a ┆ 1749256789 ┆ 3    ┆ 0    ┆ -1.0       ┆ [{\"x\": \"*\"}, {\"x\":     │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ \"*\"}, {\"x\": \"*\"}]      │\n",
       "│ 6878   ┆ 69.30.249.206          ┆ 1749256789 ┆ 3    ┆ 3    ┆ 148.248946 ┆ [{\"rtt\": 148.23621},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 148.301316},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": …              │\n",
       "│ 6878   ┆ 45.41.55.165           ┆ 1749256789 ┆ 3    ┆ 0    ┆ -1.0       ┆ [{\"x\": \"*\"}, {\"x\":     │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ \"*\"}, {\"x\": \"*\"}]      │\n",
       "│ 6878   ┆ 92.38.176.25           ┆ 1749256790 ┆ 3    ┆ 3    ┆ 126.80081  ┆ [{\"rtt\": 126.767651},  │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 126.777332},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\":…               │\n",
       "│ 6878   ┆ 212.62.68.29           ┆ 1749256793 ┆ 3    ┆ 3    ┆ 29.075086  ┆ [{\"rtt\": 29.087843},   │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 29.118559},    │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ {\"rtt\": 2…             │\n",
       "│ 6878   ┆ 34.89.240.90           ┆ 1749256797 ┆ 3    ┆ 0    ┆ -1.0       ┆ [{\"x\": \"*\"}, {\"x\":     │\n",
       "│        ┆                        ┆            ┆      ┆      ┆            ┆ \"*\"}, {\"x\": \"*\"}]      │\n",
       "└────────┴────────────────────────┴────────────┴──────┴──────┴────────────┴────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidOperationError",
     "evalue": "list.eval operation not supported for dtype `str`\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nParquet SCAN [data/ping/ping-2025-06-07T0000.parquet/part.121.parquet, ... 99 other sources] [id: 129519447274064]\nPROJECT */7 COLUMNS",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidOperationError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# For the first batch, create the file. For subsequent batches, append to it.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_batch:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mtransformed_lazy\u001b[49m\u001b[43m.\u001b[49m\u001b[43msink_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     first_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# To append, we collect the result of the small scan and write_parquet\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# This is safe because each batch is small\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/lazyframe/frame.py:2808\u001b[39m, in \u001b[36mLazyFrame.sink_parquet\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2806\u001b[39m     ldf = ldf.with_optimizations(optimizations._pyoptflags)\n\u001b[32m   2807\u001b[39m     ldf = LazyFrame._from_pyldf(ldf)\n\u001b[32m-> \u001b[39m\u001b[32m2808\u001b[39m     \u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2809\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LazyFrame._from_pyldf(ldf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/workspace/zevwilson_umass_edu-simple/ping-ingest/.venv/lib/python3.12/site-packages/polars/lazyframe/frame.py:2332\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2330\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2331\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mInvalidOperationError\u001b[39m: list.eval operation not supported for dtype `str`\n\nResolved plan until failure:\n\n\t---> FAILED HERE RESOLVING 'sink' <---\nParquet SCAN [data/ping/ping-2025-06-07T0000.parquet/part.121.parquet, ... 99 other sources] [id: 129519447274064]\nPROJECT */7 COLUMNS"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists and the old file is gone\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    os.remove(OUTPUT_PATH)\n",
    "\n",
    "first_batch = True\n",
    "for i in range(0, len(all_files), BATCH_SIZE):\n",
    "    batch_files = all_files[i : i + BATCH_SIZE]\n",
    "    print(f\"Processing batch {i // BATCH_SIZE + 1} ({len(batch_files)} files)...\")\n",
    "\n",
    "    # Scan only the files in the current batch\n",
    "    lazy_df = pl.scan_parquet(batch_files)\n",
    "    display(lazy_df.head(10).collect())\n",
    "\n",
    "    # Apply the same transformation\n",
    "    transformed_lazy = lazy_df.with_columns(\n",
    "        extract_rtts_native(n=N_RTTS)\n",
    "    ).drop(\"result\")\n",
    "\n",
    "    # For the first batch, create the file. For subsequent batches, append to it.\n",
    "    if first_batch:\n",
    "        transformed_lazy.sink_parquet(OUTPUT_PATH)\n",
    "        first_batch = False\n",
    "    else:\n",
    "        # To append, we collect the result of the small scan and write_parquet\n",
    "        # This is safe because each batch is small\n",
    "        df_batch_result = transformed_lazy.collect(streaming=True)\n",
    "        df_batch_result.write_parquet(OUTPUT_PATH, use_pyarrow=True, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328d6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read probe IP mapping CSV\n",
    "ip_map = pl.read_csv(\"probe_ip_map.csv\")\n",
    "ip_map = ip_map.rename({\"prb_id\": \"dst_prb_id\"})\n",
    "print(\"IP map sample:\")\n",
    "display(ip_map.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IP map head:\")\n",
    "display(ip_map.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92623354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Main dataframe head:\")\n",
    "display(df.head(10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef319336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join data with IP mapping\n",
    "df1 = df.rename({\"prb_id\": \"src\"})\n",
    "dfl2 = df1.join(\n",
    "    ip_map.select([\"ip\", \"dst_prb_id\"]),\n",
    "    left_on=\"dst_addr\", right_on=\"ip\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill nulls and cast to int\n",
    "dfl2 = dfl2.with_columns(\n",
    "    pl.col(\"dst_prb_id\").fill_null(-1).cast(pl.Int64)\n",
    ")\n",
    "\n",
    "print(\"After join:\")\n",
    "display(dfl2.head(10).collect())\n",
    "\n",
    "# Filter where dst_prb_id is not -1\n",
    "dfl3 = dfl2.filter(pl.col(\"dst_prb_id\") != -1)\n",
    "print(\"\\nAfter filtering:\")\n",
    "display(dfl3.head(10).collect())\n",
    "\n",
    "# Select columns and rename\n",
    "dfl6 = dfl3.select([\"src\", \"dst_prb_id\", \"ts\", \"avg\", \"result\", \"sent\", \"rcvd\"])\n",
    "dfl6 = dfl6.rename({\"dst_prb_id\": \"dst\"})\n",
    "\n",
    "print(\"\\nFinal dataframe:\")\n",
    "display(dfl6.head(10).collect())\n",
    "print(f\"Total rows: {dfl6.collect().height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique edges\n",
    "edges = dfl6.select([\"src\", \"dst\"]).unique()\n",
    "print(\"Unique edges:\")\n",
    "display(edges.head(10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d019635",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total unique edges: {edges.collect().height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count connections per source\n",
    "conn_counts = edges.group_by(\"src\").agg(\n",
    "    pl.count().alias(\"connection_count\")\n",
    ").sort(\"connection_count\", descending=True)\n",
    "\n",
    "print(\"Top 1000 sources by connection count:\")\n",
    "display(conn_counts.head(1000).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 100 sources\n",
    "top_src = conn_counts.head(100).select(\"src\").to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to top sources only\n",
    "ddf = dfl6.filter(\n",
    "    pl.col(\"src\").is_in(top_src) & pl.col(\"dst\").is_in(top_src)\n",
    ")\n",
    "\n",
    "print(\"Filtered to top sources:\")\n",
    "display(ddf.head(10).collect())\n",
    "print(f\"Total rows: {ddf.collect().height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for partial packet loss\n",
    "partial_loss = ddf.filter(\n",
    "    (pl.col(\"rcvd\") < 3) & (pl.col(\"rcvd\") > 0)\n",
    ")\n",
    "display(partial_loss.head(10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal investigation\n",
    "print(\"=== Temporal Investigation ===\")\n",
    "\n",
    "# Get timestamp statistics\n",
    "ts_stats = ddf.select(\"ts\").describe()\n",
    "print(\"Timestamp statistics:\")\n",
    "display(ts_stats.collect())\n",
    "\n",
    "# Get unique timestamps\n",
    "ts_values = ddf.select(\"ts\").unique().sort(\"ts\").collect()\n",
    "ts_values_list = ts_values[\"ts\"].to_list()\n",
    "print(f\"\\nNumber of unique timestamps: {len(ts_values_list)}\")\n",
    "\n",
    "# Calculate time differences\n",
    "if len(ts_values_list) > 1:\n",
    "    ts_diffs = [ts_values_list[i+1] - ts_values_list[i] for i in range(len(ts_values_list)-1)]\n",
    "    print(f\"Time step differences (first 10): {ts_diffs[:10]}\")\n",
    "    print(f\"Min time step: {min(ts_diffs)}\")\n",
    "    print(f\"Max time step: {max(ts_diffs)}\")\n",
    "    print(f\"Most common time step: {max(set(ts_diffs), key=ts_diffs.count)}\")\n",
    "\n",
    "print(\"\\n=== Data Completeness Investigation ===\")\n",
    "\n",
    "# Get unique values for each dimension\n",
    "unique_src = ddf.select(\"src\").unique().collect().height\n",
    "unique_dst = ddf.select(\"dst\").unique().collect().height\n",
    "unique_ts = ddf.select(\"ts\").unique().collect().height\n",
    "\n",
    "print(f\"Unique sources: {unique_src}\")\n",
    "print(f\"Unique destinations: {unique_dst}\")\n",
    "print(f\"Unique timestamps: {unique_ts}\")\n",
    "\n",
    "# Calculate theoretical vs actual data points\n",
    "theoretical_points = unique_src * unique_dst * unique_ts\n",
    "actual_points = ddf.collect().height\n",
    "print(f\"Theoretical data points: {theoretical_points:,}\")\n",
    "print(f\"Actual data points: {actual_points:,}\")\n",
    "print(f\"Data completeness: {actual_points/theoretical_points*100:.2f}%\")\n",
    "\n",
    "# Check for self-loops\n",
    "self_loops = ddf.filter(pl.col(\"src\") == pl.col(\"dst\")).collect()\n",
    "print(f\"Self-loops (src == dst): {self_loops.height}\")\n",
    "\n",
    "# Sample data structure\n",
    "print(\"\\n=== Sample Data Structure ===\")\n",
    "sample_data = ddf.select([\"src\", \"dst\", \"ts\", \"avg\"]).head(20).collect()\n",
    "print(\"Sample data:\")\n",
    "display(sample_data)\n",
    "\n",
    "# Check for multiple measurements per src-dst-ts combination\n",
    "duplicates = ddf.group_by([\"src\", \"dst\", \"ts\"]).agg(\n",
    "    pl.count().alias(\"count\")\n",
    ").collect()\n",
    "\n",
    "print(f\"\\nMultiple measurements per src-dst-ts combination:\")\n",
    "print(f\"Max measurements per combination: {duplicates['count'].max()}\")\n",
    "print(f\"Mean measurements per combination: {duplicates['count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by src, dst, ts and count\n",
    "grouped_counts = ddf.group_by([\"src\", \"dst\", \"ts\"]).agg(\n",
    "    pl.count().alias(\"count\")\n",
    ").collect()\n",
    "display(grouped_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac89de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_temporal_variation_basic_polars(df):\n",
    "    \"\"\"\n",
    "    Calculate basic temporal variation statistics using polars\n",
    "    \"\"\"\n",
    "    print(\"=== Basic Temporal Variation Analysis ===\")\n",
    "    \n",
    "    # Convert timestamp to datetime for easier analysis\n",
    "    df_with_time = df.with_columns([\n",
    "        pl.col(\"ts\").cast(pl.Datetime).alias(\"datetime\"),\n",
    "        pl.col(\"ts\").cast(pl.Datetime).dt.hour().alias(\"hour\"),\n",
    "        pl.col(\"ts\").cast(pl.Datetime).dt.weekday().alias(\"day_of_week\")\n",
    "    ]).collect()\n",
    "    \n",
    "    # Overall temporal statistics\n",
    "    min_time = df_with_time[\"datetime\"].min()\n",
    "    max_time = df_with_time[\"datetime\"].max()\n",
    "    print(f\"Time span: {min_time} to {max_time}\")\n",
    "    print(f\"Total duration: {max_time - min_time}\")\n",
    "    print(f\"Number of unique timestamps: {df_with_time['ts'].n_unique()}\")\n",
    "    \n",
    "    # Temporal distribution of measurements\n",
    "    hourly_counts = df_with_time.group_by(\"hour\").agg(\n",
    "        pl.count().alias(\"count\")\n",
    "    )\n",
    "    daily_counts = df_with_time.group_by(\"day_of_week\").agg(\n",
    "        pl.count().alias(\"count\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMeasurements per hour (mean): {hourly_counts['count'].mean():.2f}\")\n",
    "    print(f\"Measurements per hour (std): {hourly_counts['count'].std():.2f}\")\n",
    "    print(f\"Measurements per day (mean): {daily_counts['count'].mean():.2f}\")\n",
    "    \n",
    "    return df_with_time, hourly_counts, daily_counts\n",
    "\n",
    "def analyze_node_pair_temporal_variation_polars(df, top_n_pairs=10):\n",
    "    \"\"\"\n",
    "    Analyze temporal variation for specific node pairs using polars\n",
    "    \"\"\"\n",
    "    print(\"=== Node Pair Temporal Variation Analysis ===\")\n",
    "    \n",
    "    # Find most active node pairs\n",
    "    pair_counts = df.group_by([\"src\", \"dst\"]).agg(\n",
    "        pl.count().alias(\"count\")\n",
    "    ).sort(\"count\", descending=True).collect()\n",
    "    \n",
    "    top_pairs = pair_counts.head(top_n_pairs)\n",
    "    temporal_variations = {}\n",
    "    \n",
    "    for row in top_pairs.iter_rows(named=True):\n",
    "        src, dst, count = row[\"src\"], row[\"dst\"], row[\"count\"]\n",
    "        \n",
    "        # Filter data for this pair\n",
    "        pair_data = df.filter(\n",
    "            (pl.col(\"src\") == src) & (pl.col(\"dst\") == dst)\n",
    "        ).with_columns(\n",
    "            pl.col(\"ts\").cast(pl.Datetime).alias(\"datetime\")\n",
    "        ).collect()\n",
    "        \n",
    "        if pair_data.height > 0:\n",
    "            # Calculate temporal statistics\n",
    "            hourly_var = pair_data.group_by(\n",
    "                pl.col(\"datetime\").dt.hour()\n",
    "            ).agg(\n",
    "                pl.col(\"avg\").std().alias(\"std\")\n",
    "            )[\"std\"].mean()\n",
    "            \n",
    "            daily_var = pair_data.group_by(\n",
    "                pl.col(\"datetime\").dt.weekday()\n",
    "            ).agg(\n",
    "                pl.col(\"avg\").std().alias(\"std\")\n",
    "            )[\"std\"].mean()\n",
    "            \n",
    "            # Calculate coefficient of variation\n",
    "            avg_std = pair_data[\"avg\"].std()\n",
    "            avg_mean = pair_data[\"avg\"].mean()\n",
    "            cv = avg_std / avg_mean if avg_mean != 0 else 0\n",
    "            \n",
    "            # Calculate temporal autocorrelation (simplified)\n",
    "            if pair_data.height > 1:\n",
    "                sorted_data = pair_data.sort(\"ts\")\n",
    "                latencies = sorted_data[\"avg\"].to_list()\n",
    "                if len(latencies) > 2:\n",
    "                    # Simple autocorrelation calculation\n",
    "                    mean_lat = np.mean(latencies)\n",
    "                    var_lat = np.var(latencies)\n",
    "                    if var_lat > 0:\n",
    "                        autocorr = np.corrcoef(latencies[:-1], latencies[1:])[0, 1]\n",
    "                    else:\n",
    "                        autocorr = np.nan\n",
    "                else:\n",
    "                    autocorr = np.nan\n",
    "            else:\n",
    "                autocorr = np.nan\n",
    "            \n",
    "            temporal_variations[(src, dst)] = {\n",
    "                'count': count,\n",
    "                'hourly_variation': hourly_var,\n",
    "                'daily_variation': daily_var,\n",
    "                'coefficient_of_variation': cv,\n",
    "                'autocorrelation': autocorr,\n",
    "                'mean_latency': avg_mean,\n",
    "                'std_latency': avg_std\n",
    "            }\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    variation_data = []\n",
    "    for (src, dst), stats in temporal_variations.items():\n",
    "        stats['src'] = src\n",
    "        stats['dst'] = dst\n",
    "        variation_data.append(stats)\n",
    "    \n",
    "    variation_df = pl.DataFrame(variation_data)\n",
    "    \n",
    "    print(\"Temporal variation for top node pairs:\")\n",
    "    display(variation_df.round(3))\n",
    "    \n",
    "    return variation_df\n",
    "\n",
    "# Usage\n",
    "df_with_time, hourly_counts, daily_counts = calculate_temporal_variation_basic_polars(ddf)\n",
    "variation_df = analyze_node_pair_temporal_variation_polars(ddf, top_n_pairs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4516973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_time_variance_heatmaps_polars(df):\n",
    "    # Convert to pandas for matplotlib compatibility\n",
    "    df_pandas = df.collect().to_pandas()\n",
    "    \n",
    "    # Get all unique src and dst\n",
    "    srcs = sorted(df_pandas['src'].unique())\n",
    "    dsts = sorted(df_pandas['dst'].unique())\n",
    "    src_idx = {s: i for i, s in enumerate(srcs)}\n",
    "    dst_idx = {d: i for i, d in enumerate(dsts)}\n",
    "    \n",
    "    # Initialize matrices\n",
    "    stddev_matrix = np.full((len(srcs), len(dsts)), np.nan)\n",
    "    autocorr_matrix = np.full((len(srcs), len(dsts)), np.nan)\n",
    "    \n",
    "    # Group by pair and compute stats\n",
    "    for (src, dst), group in df_pandas.groupby(['src', 'dst']):\n",
    "        if len(group) > 1:\n",
    "            latencies = group.sort_values('ts')['avg'].values\n",
    "            stddev_matrix[src_idx[src], dst_idx[dst]] = np.std(latencies)\n",
    "            # Autocorrelation (lag-1)\n",
    "            if len(latencies) > 2:\n",
    "                autocorr_matrix[src_idx[src], dst_idx[dst]] = pd.Series(latencies).autocorr()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    im0 = axes[0].imshow(stddev_matrix, aspect='auto', cmap='magma')\n",
    "    axes[0].set_title('Per-Pair Latency Stddev Over Time')\n",
    "    axes[0].set_xlabel('Destination Index')\n",
    "    axes[0].set_ylabel('Source Index')\n",
    "    plt.colorbar(im0, ax=axes[0], label='Stddev (ms)')\n",
    "    \n",
    "    im1 = axes[1].imshow(autocorr_matrix, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[1].set_title('Per-Pair Latency Autocorrelation (Lag-1)')\n",
    "    axes[1].set_xlabel('Destination Index')\n",
    "    axes[1].set_ylabel('Source Index')\n",
    "    plt.colorbar(im1, ax=axes[1], label='Autocorrelation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "pairwise_time_variance_heatmaps_polars(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate latency distribution visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Find node pairs where src != dst (no self-loops) with good data coverage\n",
    "node_pairs = ddf.group_by([\"src\", \"dst\"]).agg(\n",
    "    pl.count().alias(\"count\")\n",
    ").sort(\"count\", descending=True).collect()\n",
    "\n",
    "# Filter out self-loops (where src == dst)\n",
    "valid_pairs = node_pairs.filter(pl.col(\"src\") != pl.col(\"dst\"))\n",
    "print(\"Top 10 most active node pairs (excluding self-loops):\")\n",
    "display(valid_pairs.head(10))\n",
    "\n",
    "# Select a single node pair with good data coverage\n",
    "if valid_pairs.height > 0:\n",
    "    # Get the most active pair with at least 20 measurements\n",
    "    active_pairs = valid_pairs.filter(pl.col(\"count\") >= 20)\n",
    "    if active_pairs.height > 0:\n",
    "        selected_row = active_pairs.head(1).collect()\n",
    "        src, dst = selected_row[\"src\"][0], selected_row[\"dst\"][0]\n",
    "        count = selected_row[\"count\"][0]\n",
    "        print(f\"\\nSelected node pair: {src} → {dst} with {count} measurements\")\n",
    "        \n",
    "        # Get data for this specific pair\n",
    "        pair_data = ddf.filter(\n",
    "            (pl.col(\"src\") == src) & \n",
    "            (pl.col(\"dst\") == dst) & \n",
    "            (pl.col(\"avg\") > 0)\n",
    "        ).collect()\n",
    "        \n",
    "        if pair_data.height > 0:\n",
    "            # Create simple histogram\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(pair_data[\"avg\"].to_list(), bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "            plt.xlabel('Average Latency (ms)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(f'Latency Distribution: Node {src} → Node {dst}\\n({pair_data.height} measurements)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics as text\n",
    "            mean_latency = pair_data[\"avg\"].mean()\n",
    "            median_latency = pair_data[\"avg\"].median()\n",
    "            std_latency = pair_data[\"avg\"].std()\n",
    "            min_latency = pair_data[\"avg\"].min()\n",
    "            max_latency = pair_data[\"avg\"].max()\n",
    "            \n",
    "            stats_text = f'Mean: {mean_latency:.2f} ms\\n'\n",
    "            stats_text += f'Median: {median_latency:.2f} ms\\n'\n",
    "            stats_text += f'Std Dev: {std_latency:.2f} ms\\n'\n",
    "            stats_text += f'Range: {min_latency:.2f} - {max_latency:.2f} ms'\n",
    "            \n",
    "            plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nStatistics for Node {src} → Node {dst}:\")\n",
    "            print(f\"Number of measurements: {pair_data.height}\")\n",
    "            print(f\"Mean latency: {mean_latency:.2f} ms\")\n",
    "            print(f\"Median latency: {median_latency:.2f} ms\")\n",
    "            print(f\"Standard deviation: {std_latency:.2f} ms\")\n",
    "            print(f\"Min latency: {min_latency:.2f} ms\")\n",
    "            print(f\"Max latency: {max_latency:.2f} ms\")\n",
    "        else:\n",
    "            print(\"No successful measurements found for this pair\")\n",
    "    else:\n",
    "        print(\"No node pairs found with sufficient data (>= 20 measurements)\")\n",
    "else:\n",
    "    print(\"No valid node pairs found (all are self-loops)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
