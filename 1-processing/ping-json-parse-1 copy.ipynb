{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dask.bag as db\n",
        "import json\n",
        "from dask.distributed import Client\n",
        "import dask.dataframe as dd\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client()\n",
        "client.dashboard_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bz2\n",
        "from typing import Iterator\n",
        "\n",
        "def decompress_partition(blocks: Iterator[bytes]) -> Iterator[str]:\n",
        "    dec = bz2.BZ2Decompressor()                # Stream‚Äêfriendly decompressor :contentReference[oaicite:8]{index=8}\n",
        "    buffer = b\"\"\n",
        "    for chunk in blocks:\n",
        "        buffer += dec.decompress(chunk)        # chunk is guaranteed bytes\n",
        "        parts = buffer.split(b\"\\n\")\n",
        "        buffer = parts.pop()                   # keep the trailing partial line\n",
        "        for line in parts:\n",
        "            yield line.decode(\"utf-8\")\n",
        "    if buffer:\n",
        "        yield buffer.decode(\"utf-8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import dask\n",
        "import dask.bag as db\n",
        "import dask.bytes as dbbytes\n",
        "\n",
        "# 1. Read raw BZ2 bytes (~1 MiB blocks)\n",
        "compressed_blocks, _ = dbbytes.read_bytes(\n",
        "    \"data/ping-2025-06-10T0000.bz2\",\n",
        "    blocksize=\"1 MiB\",\n",
        "    compression=None\n",
        ")\n",
        "\n",
        "bag_blocks = db.from_sequence(compressed_blocks, partition_size=1)\n",
        "\n",
        "# 2. Wrap each block in the streaming decompressor\n",
        "lines = bag_blocks.map_partitions(decompress_partition)  # Returns a Bag of text lines :contentReference[oaicite:9]{index=9}\n",
        "\n",
        "# 3. Build a Bag from these delayed lists of lines\n",
        "bag = db.from_delayed(lines)\n",
        "\n",
        "# 4. Parse JSON and filter by your timestamp\n",
        "records  = bag.map(json.loads)\n",
        "filtered = records.filter(lambda rec: rec.get(\"sent\", -1) == 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_parquet(\"data/ping-2025-06-10-0h-lat\", engine=\"pyarrow\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
